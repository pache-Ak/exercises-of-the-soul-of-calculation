# 第1章　毫厘千里之差——大O概念

---

## 1.1　算法的规范化和量化度量

### 笔记

#### 早期计算机到通用计算机

早期计算机不区分硬件和软件， 事实上导致它成了专有计算机， 用于处理特定问题。
想要处理新的问题不得不修改电路， 有着可怕的工作量和极低的效率

冯·诺伊曼等人将程序从计算机中分离出来， 客观上将计算机分成硬件和软件两部分
新的体系结构包含了存储、计算等部分， 将控制程序分离了出来。
修改控制程序比修改电路要轻松的多， 新的计算机可以个恒功的处理各种问题， 只需要设计新的程序即可。

而冯·诺伊曼体系的计算机在硬件方面恰到好处的实现了对现实的抽象， 一步登峰造极难以突破

#### 程序 算法 高德纳

至此， 工作的重点从不区分软硬件的计算机到程序设计上。

程序设计的核心正是算法， 这方面领域的空白被高德纳填补。

#### 计算机领域的设计 自顶向下

### 思考题 1.1

世界上还有什么产品类似于计算机，是软硬件分离的？（ 难度系数1颗星）

智能终端： 智能手机 手表 电视 智能化家具

## 1.2　大数和数量级的概念

### 笔记

#### 大数

人对大数没有直观的认知， 人对计算机资源也没有直观的认知。

计算机不是速度无限大， 内存无限大， 有必要节省计算机资源。

计算机主要用于处理大数据， 数据规模难以想象的大，

影响算法快慢的两类因素:

  1. 不随数据量变化的因素
  2. 随数据量变化的因素

其中 随数据量变化的因素影响更为重要。

所以考虑算法的时候只专注于数据无限大的情况。

#### 大O

数学上对大O概念的定义，
如果两个函数f(N)和g(N)，
在N趋近于无穷大时比值只差一个常数，
那么它们就被看成同一个数量级的函数。

而在计算机科学中相应的算法，也就被认为是具有相同的复杂度。

如果一个算法的复杂度由一高一低的两部分f(N)和g(N)组成，即f(N)+g(N)，
后面数量级低的那部分可以直接省略，也就是说O(f(N)+g(N))=O(f(N))。
这在数学上显然不成立，但是在计算机算法上是被认可的。

### 思考题1.2

如果一个程序只运行一次，在编写它的时候，你是采用最直观但是效率较低的算法，还是依然寻找复杂度最优的算法？（难度系数2颗星）

此时需要考虑两方面成本， 开发成本和运行成本。

因为只运行一次， 开发成本成为相当大的一部分因素。

此时对于程序数据量应当有所估计， 对于较为低效的程序的运行时间能否接受也可做出估计。

考虑寻找最优算法所需的成本， 和可能出现错误需要反复debug的成本， 较长的运行时间不是难以接受。

将两方面成本相比较， 运行时间更容易估计， 花费时间长段经过考虑也可以接受， 开发时间更难以估计， 更难以保证正确性。

所以采用最直观但是效率较的算法是更可行的选择。
